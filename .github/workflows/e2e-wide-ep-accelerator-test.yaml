name: E2E Wide EP Test

on:
  issue_comment:
    # Runs with a PR comment /run-e2e-wide-ep
    types: [created]
  workflow_dispatch:
    inputs:
      pr_or_branch:
        description: 'Pull-request number or branch name to test'
        required: true
        default: 'main'
        type: string
      gateway_type:
        description: 'Gateway type to use'
        required: false
        default: 'istio' # currently only istio supported because thats the default used in infra values
        type: choice
        options:
          - istio
      wait_for_termination:
        description: 'Wait time (in minutes) before terminating for debugging'
        required: true
        default: 0
        type: number

permissions:
  packages: read

env:
  QUICKSTART_PATH: "quickstart"
  WIDE_EP_PATH: "quickstart/examples/wide-ep-lws"

jobs:
  setup:
    if: >
      github.event_name == 'workflow_dispatch' ||
      (
        github.event_name == 'issue_comment' &&
        github.event.issue.pull_request &&
        (
          contains(github.event.comment.body, '/run-e2e-wide-ep')
        ) &&
        (
          github.event.comment.author_association == 'OWNER' ||
          github.event.comment.author_association == 'MEMBER' ||
          github.event.comment.author_association == 'COLLABORATOR'
        )
      )
    runs-on: ubuntu-latest
    outputs:
      instance_id: ${{ steps.launch.outputs.instance_id }}
      instance_ip: ${{ steps.launch.outputs.instance_ip }}
    env:
      # g6.12xlarge-4xL4-24GB-GPU
      INSTANCE_TYPE: g6.12xlarge
      AMI_ID: ami-020cba7c55df1f615
      KEY_NAME: ${{ secrets.SSH_KEY_NAME }}
      REGION: ${{ secrets.AWS_REGION }}
      HF_TOKEN: ${{secrets.HF_TOKEN}}
      TERMINATION_TIMEOUT: ${{ github.event.inputs.wait_for_termination }}
      DISK_SIZE: "300"
      PR_OR_BRANCH: ${{ github.event.inputs.pr_or_branch || github.event.issue.number }}
      NAMESPACE: ${{ github.event.inputs.namespace }}
      GATEWAY_TYPE: ${{ github.event.inputs.gateway_type || 'istio' }}

    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          persist-credentials: false

      - name: Determine if pr_or_branch is a PR number
        id: check_pr
        run: |
          if [[ "$PR_OR_BRANCH" =~ ^[0-9]+$ ]]; then
            echo "is_pr=true" >> "$GITHUB_OUTPUT"
          else
            echo "is_pr=false" >> "$GITHUB_OUTPUT"
          fi

      - name: Fetch and checkout PR
        if: steps.check_pr.outputs.is_pr == 'true'
        run: |
          git fetch origin pull/"$PR_OR_BRANCH"/head:pr-"$PR_OR_BRANCH"
          git checkout pr-"$PR_OR_BRANCH"

      - name: Checkout branch
        if: steps.check_pr.outputs.is_pr == 'false'
        run: git checkout "$PR_OR_BRANCH"

      - name: Install AWS CLI
        run: |
          sudo apt-get update
          curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
          unzip awscliv2.zip
          sudo ./aws/install --update

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@7474bc4690e29a8392af63c5b98e7449536d5c3a
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_REGION }}

      - name: Launch EC2 instance
        id: launch
        run: |
          INSTANCE_ID=$(aws ec2 run-instances \
            --image-id $AMI_ID \
            --count 1 \
            --instance-type $INSTANCE_TYPE \
            --key-name $KEY_NAME \
            --block-device-mappings "[{
                \"DeviceName\": \"/dev/sda1\",
                \"Ebs\": {
                  \"VolumeSize\": ${DISK_SIZE},
                  \"VolumeType\": \"gp3\",
                  \"DeleteOnTermination\": true
                }
              }]" \
            --tag-specifications "ResourceType=instance,Tags=[{Key=Name,Value=llmd-wide-ep-ci-runner}]" \
            --query 'Instances[0].InstanceId' \
            --output text)

          echo "instance_id=$INSTANCE_ID" >> "$GITHUB_OUTPUT"
          echo "INSTANCE_ID=$INSTANCE_ID" >> $GITHUB_ENV

          echo "Waiting for instance to be running..."
          aws ec2 wait instance-running --instance-ids $INSTANCE_ID

          PUBLIC_IP=$(aws ec2 describe-instances \
            --instance-ids $INSTANCE_ID \
            --query "Reservations[0].Instances[0].PublicIpAddress" \
            --output text)

          echo "instance_ip=$PUBLIC_IP" >> "$GITHUB_OUTPUT"
          echo "INSTANCE_IP=$PUBLIC_IP" >> $GITHUB_ENV

          SECURITY_GROUP_ID=$(aws ec2 describe-instances \
            --instance-ids $INSTANCE_ID \
            --query "Reservations[0].Instances[0].SecurityGroups[0].GroupId" \
            --output text)
          echo "Authorizing SSH in security group $SECURITY_GROUP_ID..."
          aws ec2 authorize-security-group-ingress \
            --group-id $SECURITY_GROUP_ID \
            --protocol tcp \
            --port 22 \
            --cidr 0.0.0.0/0 || echo "SSH rule may already exist ‚Äî continuing"

      - name: Wait for SSH to be ready
        run: |
          echo "${{ secrets.SSH_PRIVATE_KEY }}" > key.pem
          chmod 600 key.pem

          echo "Waiting for SSH on $INSTANCE_IP..."
          for i in {1..30}; do
            ssh -o StrictHostKeyChecking=no -i key.pem ubuntu@$INSTANCE_IP "echo connected" && break
            sleep 10
          done

      - name: Setup installer pre-requisites (clone + checkout)
        id: setup-pre-requisite
        run: |
          # pass PR_OR_BRANCH into the remote shell's env, keep heredoc single‚Äëquoted
          ssh -o StrictHostKeyChecking=no -i key.pem \
              ubuntu@$INSTANCE_IP \
              "PR_OR_BRANCH=$PR_OR_BRANCH bash -s" <<'EOF'
            set -euo pipefail
            set -x

            sudo apt-get update -y
            sudo apt-get install -y git

            # Install yq for YAML processing
            echo "Installing yq..."
            sudo wget https://github.com/mikefarah/yq/releases/latest/download/yq_linux_amd64 -O /usr/local/bin/yq && \
            sudo chmod +x /usr/local/bin/yq
            yq --version

            REPO_URL="https://github.com/llm-d-incubation/llm-d-infra.git"
            REPO_DIR=$(basename "$REPO_URL" .git)

            echo "üõ†Ô∏è  Cloning: $REPO_URL"
            git clone --depth 1 "$REPO_URL"
            cd "$REPO_DIR"

            if [[ "$PR_OR_BRANCH" =~ ^[0-9]+$ ]]; then
              echo "üõ†Ô∏è  Checking out PR #$PR_OR_BRANCH"
              git fetch origin "pull/$PR_OR_BRANCH/head:pr-$PR_OR_BRANCH"
              git checkout "pr-$PR_OR_BRANCH"
            else
              echo "üõ†Ô∏è  Checking out branch $PR_OR_BRANCH"
              git checkout "$PR_OR_BRANCH"
            fi
          EOF

      - name: Apply Wide EP slim transformation
        id: wide-ep-transformation
        run: |
          ssh -o StrictHostKeyChecking=no -i key.pem ubuntu@$INSTANCE_IP <<'EOF'
            set -euo pipefail
            set -x
            cd llm-d-infra

            echo "Applying wide-ep slim values transform..."

            # Transform ms-wide-ep/values.yaml to a scaled-down deployment
            # Need a small MoE model for this example: Qwen1.5-MoE-A2.7B-Chat.
            # This example will be a 2 by 2 (1 replica, DP of 2 for decode, both using 2 GPUs)

            export NEW_MODEL_NAME="Qwen/Qwen1.5-MoE-A2.7B"
            export OLD_MODEL_NAME="deepseek-ai/DeepSeek-R1-0528"

            yq e '.modelArtifacts.uri = "hf://'${NEW_MODEL_NAME}'"' -i ${{ env.WIDE_EP_PATH }}/ms-wide-ep/values.yaml
            yq e '.modelArtifacts.size = "30Gi"' -i ${{ env.WIDE_EP_PATH }}/ms-wide-ep/values.yaml
            yq e '.routing.modelName = "'${NEW_MODEL_NAME}'"' -i ${{ env.WIDE_EP_PATH }}/ms-wide-ep/values.yaml

            ## Decode specific changes
            ### Unbound the accelerator type from H200
            yq e 'del(.decode.acceleratorTypes.labelValues)' -i ${{ env.WIDE_EP_PATH }}/ms-wide-ep/values.yaml

            ### Swap the model name in custom startup script
            decode_args=$(yq '.decode.containers[0].args' ${{ env.WIDE_EP_PATH }}/ms-wide-ep/values.yaml)
            export decode_args_updated=$(echo "${decode_args}" | sed 's/deepseek-ai\/DeepSeek-R1-0528/Qwen\/Qwen1.5-MoE-A2.7B-Chat/g')
            yq e '.decode.containers[0].args = strenv(decode_args_updated)' -i ${{ env.WIDE_EP_PATH }}/ms-wide-ep/values.yaml

            ### See above, example is a 2 by 2
            yq e '(.decode.containers[0].env[] | select(.name == "DP_SIZE_LOCAL")).value = "2"' -i ${{ env.WIDE_EP_PATH }}/ms-wide-ep/values.yaml

            ### The example is set to work out of the box on the coreweave cluster loading model from node storage. Were going to use HF download instead
            yq 'del(.decode.containers[0].env[] | select(.name == "HF_HUB_CACHE"))' -i ${{ env.WIDE_EP_PATH }}/ms-wide-ep/values.yaml
            yq 'del(.decode.containers[0].env[] | select(.name == "HF_HUB_DISABLE_XET"))' -i ${{ env.WIDE_EP_PATH }}/ms-wide-ep/values.yaml

            ### See above, example is a 2 by 2
            yq e '
            (.decode.containers[0].resources = {}) |
            (.decode.containers[0].resources.limits = {"nvidia.com/gpu": 2}) |
            (.decode.containers[0].resources.requests = {"nvidia.com/gpu": 2})
            ' -i ${{ env.WIDE_EP_PATH }}/ms-wide-ep/values.yaml

            ### Using model from HF rather than host storage, already discussed
            yq e ' (.decode.containers[0].mountModelVolume = true)' -i ${{ env.WIDE_EP_PATH }}/ms-wide-ep/values.yaml

            ### The example is set to work out of the box on the coreweave cluster loading model from node storage. Were going to use HF download instead.
            ### Thus we can remove the HF Cache volume mounts and let HF take care of a default cache path.
            yq e '
            .decode.containers[0].volumeMounts
                |= map(select(.name != "hf-cache"))
            |
            .decode.volumes
                |= map(select(.name != "hf-cache"))
            ' -i ${{ env.WIDE_EP_PATH }}/ms-wide-ep/values.yaml

            ## Prefill specific changes
            ### Unbound the accelerator type from H200
            yq e 'del(.prefill.acceleratorTypes.labelValues)' -i ${{ env.WIDE_EP_PATH }}/ms-wide-ep/values.yaml

            ### Swap the model name in custom startup script
            prefill_args=$(yq '.prefill.containers[0].args' ${{ env.WIDE_EP_PATH }}/ms-wide-ep/values.yaml)
            export prefill_args_updated=$(echo "${prefill_args}" | sed 's/deepseek-ai\/DeepSeek-R1-0528/Qwen\/Qwen1.5-MoE-A2.7B-Chat/g')
            yq e '.prefill.containers[0].args = strenv(prefill_args_updated)' -i ${{ env.WIDE_EP_PATH }}/ms-wide-ep/values.yaml

            ### See above, example is a 2 by 2
            yq e '(.prefill.containers[0].env[] | select(.name == "DP_SIZE_LOCAL")).value = "2"' -i ${{ env.WIDE_EP_PATH }}/ms-wide-ep/values.yaml

            ### The example is set to work out of the box on the coreweave cluster loading model from node storage. Were going to use HF download instead
            yq 'del(.prefill.containers[0].env[] | select(.name == "HF_HUB_CACHE"))' -i ${{ env.WIDE_EP_PATH }}/ms-wide-ep/values.yaml
            yq 'del(.prefill.containers[0].env[] | select(.name == "HF_HUB_DISABLE_XET"))' -i ${{ env.WIDE_EP_PATH }}/ms-wide-ep/values.yaml

            ### See above, example is a 2 by 2
            yq e '
            (.prefill.containers[0].resources = {}) |
            (.prefill.containers[0].resources.limits = {"nvidia.com/gpu": 2}) |
            (.prefill.containers[0].resources.requests = {"nvidia.com/gpu": 2})
            ' -i ${{ env.WIDE_EP_PATH }}/ms-wide-ep/values.yaml

            ### Using model from HF rather than host storage, already discussed
            yq e ' (.prefill.containers[0].mountModelVolume = true)' -i ${{ env.WIDE_EP_PATH }}/ms-wide-ep/values.yaml

            ### The example is set to work out of the box on the coreweave cluster loading model from node storage. Were going to use HF download instead.
            ### Thus we can remove the HF Cache volume mounts and let HF take care of a default cache path.
            yq e '
            .prefill.containers[0].volumeMounts
                |= map(select(.name != "hf-cache"))
            |
            .prefill.volumes
                |= map(select(.name != "hf-cache"))
            ' -i ${{ env.WIDE_EP_PATH }}/ms-wide-ep/values.yaml

            # Verify the slimmed down values
            echo "üìã Verifying transformation results..."
            echo "Model in ms-wide-ep:"
            yq e '.modelArtifacts.uri' ${{ env.WIDE_EP_PATH }}/ms-wide-ep/values.yaml
            echo "GPU count in ms-wide-ep decode:"
            yq e '.decode.containers[0].resources.limits["nvidia.com/gpu"]' ${{ env.WIDE_EP_PATH }}/ms-wide-ep/values.yaml
            echo "Replicas in ms-wide-ep prefill:"
            yq e '.prefill.replicas' ${{ env.WIDE_EP_PATH }}/ms-wide-ep/values.yaml

            # Validate YAML syntax
            echo "Validating YAML syntax..."
            yq eval '.' ${{ env.WIDE_EP_PATH }}/ms-wide-ep/values.yaml > /dev/null && echo "ms-wide-ep/values.yaml: Valid"
            echo "‚úÖ All YAML files are valid"

            # Check for malformed resource quantities
            echo "üîç Checking resource quantities format..."
            echo "Decode GPU limits:"
            yq e '.decode.containers[0].resources.limits | keys' ${{ env.WIDE_EP_PATH }}/ms-wide-ep/values.yaml
            echo "Decode GPU requests:"
            yq e '.decode.containers[0].resources.requests | keys' ${{ env.WIDE_EP_PATH }}/ms-wide-ep/values.yaml
            echo "Prefill GPU limits:"
            yq e '.prefill.containers[0].resources.limits | keys' ${{ env.WIDE_EP_PATH }}/ms-wide-ep/values.yaml
            echo "Prefill GPU requests:"
            yq e '.prefill.containers[0].resources.requests | keys' ${{ env.WIDE_EP_PATH }}/ms-wide-ep/values.yaml

            # Display complete transformed files for debugging
            echo ""
            echo "üîç === Completed transformed values files ==="
            echo ""
            echo "ms-wide-ep/values.yaml (transformed):"
            echo "---"
            cat ${{ env.WIDE_EP_PATH }}/ms-wide-ep/values.yaml
            echo "---"
            echo ""
            echo "infra-wide-ep/values.yaml:"
            echo "---"
            cat ${{ env.WIDE_EP_PATH }}/infra-wide-ep/values.yaml
            echo "---"
            echo ""
            echo "=== End of values files output ==="
          EOF

      - name: Run quickstart install-deps
        id: quickstart-deps
        run: |
          ssh -o StrictHostKeyChecking=no -i key.pem ubuntu@$INSTANCE_IP <<'EOF'
            set -euo pipefail
            set -x
            cd llm-d-infra/quickstart
            ./install-deps.sh | tee ~/install-deps.log
          EOF

      - name: Setup container runtime
        id: setup-docker
        run: |
          ssh -o StrictHostKeyChecking=no -i key.pem ubuntu@$INSTANCE_IP << EOF
            set -e
            sudo apt-get -y install ca-certificates curl
            sudo install -m 0755 -d /etc/apt/keyrings
            sudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc
            sudo chmod a+r /etc/apt/keyrings/docker.asc
            echo \
              "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu \
              $(. /etc/os-release && echo "${UBUNTU_CODENAME:-$VERSION_CODENAME}") stable" | \
              sudo tee /etc/apt/sources.list.d/docker.list > /dev/null

              sudo apt-get update
              sudo apt-get -y install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin
              sudo usermod -aG docker ubuntu
              mkdir -p ~/.config/containers/
          EOF

      - name: Copy docker auth configuration file
        id: docker-auth
        run: |
          echo "${{ secrets.CR_AUTH_JSON }}" > auth.json
          chmod +x auth.json
          rsync -avz -e "ssh -o StrictHostKeyChecking=no -i key.pem" auth.json ubuntu@$INSTANCE_IP:~/
          ssh -o StrictHostKeyChecking=no -i key.pem ubuntu@$INSTANCE_IP << EOF
          mv ~/auth.json ~/.config/containers/
          EOF

      - name: Setup nvidia cuda toolkit
        id: setup-cuda-toolkit
        run: |
          ssh -o StrictHostKeyChecking=no -i key.pem ubuntu@$INSTANCE_IP << EOF
            set -e
            wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2404/x86_64/cuda-keyring_1.1-1_all.deb
            sudo dpkg -i cuda-keyring_1.1-1_all.deb
            sudo apt-get update
            sudo apt-get -y install cuda-toolkit-12-8
            sudo apt-get install -y nvidia-open nvtop nload
          EOF

      - name: Reboot the aws instance
        id: reboot-instance
        run: |
          echo "Rebooting instance..."
          aws ec2 reboot-instances --instance-ids $INSTANCE_ID
          sleep 60
          echo "Waiting for instance to become healthy again..."
          aws ec2 wait instance-status-ok --instance-ids $INSTANCE_ID

      - name: Wait for SSH to be ready after reboot
        run: |
          echo "${{ secrets.SSH_PRIVATE_KEY }}" > key.pem
          chmod 600 key.pem

          echo "Waiting for SSH on $INSTANCE_IP..."
          for i in {1..30}; do
            ssh -o StrictHostKeyChecking=no -i key.pem ubuntu@$INSTANCE_IP "echo connected" && break
            sleep 10
          done

      - name: Setup nvidia container toolkit
        id: setup-container-toolkit
        run: |
          ssh -o StrictHostKeyChecking=no -i key.pem ubuntu@$INSTANCE_IP << EOF
            set -e
            curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg \
              && curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | \
                sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \
                sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list

            sudo apt-get update
            sudo apt-get install -y nvidia-container-toolkit

            sudo sysctl net.core.bpf_jit_harden
            echo "net.core.bpf_jit_harden=0" | sudo tee -a /etc/sysctl.conf
            sudo sysctl -p
            sudo nvidia-ctk runtime configure --runtime=docker && sudo systemctl restart docker
          EOF

      - name: Install minikube
        id: install-minikube
        run: |
          ssh -o StrictHostKeyChecking=no -i key.pem ubuntu@$INSTANCE_IP << EOF
            set -e
            curl -LO https://github.com/kubernetes/minikube/releases/latest/download/minikube-linux-amd64
            sudo install minikube-linux-amd64 /usr/local/bin/minikube && rm minikube-linux-amd64
          EOF

      - name: Start Shared Minikube Cluster
        run: |
          ssh -o StrictHostKeyChecking=no -i key.pem ubuntu@${{ env.INSTANCE_IP }} <<'EOF'
            set -euo pipefail
            set -x
            echo "Starting minikube with gpu support enabled..."
            minikube start --driver docker --container-runtime docker --gpus all --memory no-limit
            sleep 10
            echo "‚úÖ Minikube started."
          EOF

  deploy-and-validate:
    needs: setup
    runs-on: ubuntu-latest
    env:
      INSTANCE_IP: ${{ needs.setup.outputs.instance_ip }}
      HF_TOKEN: ${{ secrets.HF_TOKEN }}
      NAMESPACE: llm-d-wide-ep
      GATEWAY_TYPE: ${{ github.event.inputs.gateway_type || 'istio' }}

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          persist-credentials: false

      - name: Setup SSH Key
        run: |
          echo "${{ secrets.SSH_PRIVATE_KEY }}" > key.pem
          chmod 600 key.pem

      - name: Run installer to deploy llm-d infrastructure
        run: |
          ssh -o StrictHostKeyChecking=no -i key.pem ubuntu@$INSTANCE_IP "HF_TOKEN=$HF_TOKEN GATEWAY_TYPE=$GATEWAY_TYPE bash -s" <<'EOF'
            set -euo pipefail
            set -x
            cd llm-d-infra/quickstart
            echo "Deploying llm-d infrastructure for Wide EP into namespace: llm-d-wide-ep ..."
            ./llmd-infra-installer.sh \
                --namespace "llm-d-wide-ep" \
                -r infra-wide-ep \
                -f examples/wide-ep-lws/infra-wide-ep/values.yaml \
                --disable-metrics-collection | tee ~/llmd-wide-ep-installer.log
          EOF

      - name: Install LeaderWorkerSet helm chart and wait for controller
        run: |
          ssh -o StrictHostKeyChecking=no -i key.pem ubuntu@$INSTANCE_IP << EOF
            set -euo pipefail
            set -x
            export LWS_CHART_VERSION=0.7.0
            helm install lws oci://registry.k8s.io/lws/charts/lws \
              --version=${LWS_CHART_VERSION} \
              --namespace lws-system \
              --create-namespace \
              --wait --timeout 300s

            kubectl wait deploy/lws-controller-manager -n lws-system --for=condition=available --timeout=5m
          EOF

      - name: Deploy Wide EP example
        run: |
          ssh -o StrictHostKeyChecking=no -i key.pem ubuntu@$INSTANCE_IP << EOF
            set -euo pipefail
            set -x
            cd llm-d-infra/${{ env.WIDE_EP_PATH }}
            echo "Deploying Wide EP example with slim configuration..."
            helmfile --selector managedBy=helmfile apply -f helmfile.yaml --skip-diff-on-install | tee ~/wide-ep-deployment.log
          EOF

      - name: Wait for all pods to be ready
        run: |
          echo "‚è≥ Waiting for all pods in namespace to become ready..."
          ssh -o StrictHostKeyChecking=no -i key.pem ubuntu@$INSTANCE_IP "NAMESPACE=$NAMESPACE bash -s" <<'EOF'
            set -euo pipefail
            kubectl wait pod \
              --for=condition=Ready \
              --all \
              -n "${NAMESPACE}" \
              --timeout=10m
            sleep 180 # Allow extra time for model loading in Wide EP setup
            echo "‚úÖ All pods are ready."
            kubectl get pods -n "${NAMESPACE}"
          EOF

      - name: Show deployment status
        run: |
          ssh -o StrictHostKeyChecking=no -i key.pem ubuntu@$INSTANCE_IP <<EOF
            set -euo pipefail
            echo "=== Pods ==="
            kubectl get pods -n "${NAMESPACE}"
            echo ""
            echo "=== Services ==="
            kubectl get svc -n "${NAMESPACE}"
            echo ""
            echo "=== Helm releases ==="
            helm list -n "${NAMESPACE}" || true
            echo ""
            echo "=== Wide-EP-specific resources ==="
            echo "Inference Models:"
            kubectl get inferencemodels -n "${NAMESPACE}" || true
            echo "Inference Pools:"
            kubectl get inferencepools -n "${NAMESPACE}" || true
            echo "HTTP Routes:"
            kubectl get httproutes -n "${NAMESPACE}" || true
          EOF

      - name: Apply Istio DestinationRule
        if: ${{ env.GATEWAY_TYPE == 'istio' }}
        run: |
          set -euo pipefail
          NS="${NAMESPACE}"

          echo "üîé Locating *-epp service in namespace $NS ‚Ä¶"
          EPP_NAME=$(ssh -o StrictHostKeyChecking=no -i key.pem ubuntu@$INSTANCE_IP \
              kubectl get svc -n "$NS" -o custom-columns=:metadata.name |
              tr ' ' '\n' | grep -- '-epp$' | head -n1 || true)

          echo "EPP_NAME = ${EPP_NAME:-<none found>}"

          if [[ -z "$EPP_NAME" ]]; then
            echo "‚ùå No Service ending in -epp found; aborting"
            exit 1
          fi

          export EPP_NAME
          TEMPLATE="$GITHUB_WORKSPACE/.github/manifests/istio-tls-destinationrule.yaml"
          envsubst < "$TEMPLATE" > destinationrule.yaml

          echo "üìù Rendered DestinationRule:"
          cat destinationrule.yaml

          # Copy and apply on the remote cluster
          scp -o StrictHostKeyChecking=no -i key.pem destinationrule.yaml \
              ubuntu@$INSTANCE_IP:/tmp/destinationrule.yaml
          ssh -o StrictHostKeyChecking=no -i key.pem ubuntu@$INSTANCE_IP \
              "kubectl apply -n \"$NS\" -f /tmp/destinationrule.yaml && rm /tmp/destinationrule.yaml"

      - name: Wide EP inference test
        run: |
          ssh -o StrictHostKeyChecking=no -i key.pem ubuntu@"$INSTANCE_IP" <<EOF
            set -euo pipefail
            set -x
            cd llm-d-infra/.github/scripts/e2e

            echo "üß™ Running Wide EP specific tests..."
            # Test the specific model and endpoints for Wide EP setup
            ./e2e-validate.sh -n "${NAMESPACE}"

            # Additional Wide EP validation
            echo "üîç Verifying Wide EP specific functionality..."

            # Check that we have both prefill and decode pods
            PREFILL_PODS=\$(kubectl get pods -n "${NAMESPACE}" -l app.kubernetes.io/component=prefill --no-headers | wc -l)
            DECODE_PODS=\$(kubectl get pods -n "${NAMESPACE}" -l app.kubernetes.io/component=decode --no-headers | wc -l)

            echo "Prefill pods: \$PREFILL_PODS"
            echo "Decode pods: \$DECODE_PODS"

            if [ "\$PREFILL_PODS" -lt 2 ] || [ "\$DECODE_PODS" -lt 2 ]; then
              echo "‚ùå Missing prefill or decode pods for Wide EP setup"
              exit 1
            fi

            echo "‚úÖ Wide EP validation completed successfully"
          EOF

      - name: Collect and upload Kubernetes pod logs
        if: always()
        run: |
          echo "Collecting logs for namespace: ${NAMESPACE}"
          REMOTE_TARBALL="pod-logs-wide-ep.tar.gz"
          ssh -o StrictHostKeyChecking=no -i key.pem ubuntu@$INSTANCE_IP << EOF
            mkdir -p pod-logs-wide-ep
            cd pod-logs-wide-ep
            echo "Fetching ${NAMESPACE} pods log..."
            kubectl get pods -n "${NAMESPACE}" --no-headers -o custom-columns=":metadata.name" \
            | xargs -I{} sh -c 'kubectl logs --all-containers=true -n "${NAMESPACE}" {} > "{}.log" 2>&1'
            echo "Fetching ${NAMESPACE} pods descriptions..."
            kubectl get pods -n "${NAMESPACE}" --no-headers -o custom-columns=":metadata.name" \
            | xargs -I{} sh -c 'kubectl describe pod -n "${NAMESPACE}" {} > "{}-describe.log" 2>&1'
            mv ~/llmd-wide-ep-installer.log . || true
            mv ~/wide-ep-deployment.log . || true
            mv ~/install-deps.log . || true
            cd ..
            tar -czf "$REMOTE_TARBALL" pod-logs-wide-ep
          EOF
          scp -o StrictHostKeyChecking=no -i key.pem ubuntu@$INSTANCE_IP:"$REMOTE_TARBALL" .
          mkdir -p extracted-logs-wide-ep
          tar -xzf "$REMOTE_TARBALL" -C extracted-logs-wide-ep
          echo "Logs downloaded from the AWS instance."

      - name: Upload pod logs as artifact
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: llmd-pod-logs-wide-ep
          path: extracted-logs-wide-ep

  terminate:
    needs: [setup, deploy-and-validate]
    if: always()
    runs-on: ubuntu-latest
    env:
      INSTANCE_ID: ${{ needs.setup.outputs.instance_id }}
      REGION: ${{ secrets.AWS_REGION }}
      TERMINATION_TIMEOUT: ${{ github.event.inputs.wait_for_termination }}

    steps:
      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@7474bc4690e29a8392af63c5b98e7449536d5c3a
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_REGION }}

      - name: Pause before termination (debug window)
        if: env.TERMINATION_TIMEOUT != '0'
        run: |
          echo "‚è≥  Debug pause enabled for $TERMINATION_TIMEOUT minute(s)‚Ä¶"
          for ((i=1; i<=TERMINATION_TIMEOUT; i++)); do
            printf "  ‚è≥  %02d/%02d minute(s) elapsed\n" "$i" "$TERMINATION_TIMEOUT"
            sleep 60
          done

      - name: Terminate EC2 instance
        run: |
          echo "Terminating instance $INSTANCE_ID..."
          aws ec2 terminate-instances --instance-ids $INSTANCE_ID
          echo "Waiting for instance to be terminated..."
          aws ec2 wait instance-terminated --instance-ids $INSTANCE_ID
